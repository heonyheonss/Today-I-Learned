import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data, Batch
from torch_geometric.nn import TransformerConv, global_mean_pool
from rdkit import Chem
import numpy as np

# --- 1. SMILESë¥¼ ê·¸ë˜í”„ë¡œ ë³€í™˜í•˜ëŠ” ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ---
# ë…¼ë¬¸ì˜ 2.4, 2.5 ì„¹ì…˜ì— í•´ë‹¹í•˜ëŠ” ë¶€ë¶„ì…ë‹ˆë‹¤.

def get_atom_features(atom):
    """ì›ì(Node)ì˜ íŠ¹ì§•(Feature)ì„ ì¶”ì¶œí•˜ì—¬ ë²¡í„°ë¡œ ë³€í™˜"""
    # ë…¼ë¬¸ì—ì„œëŠ” 57ì°¨ì› ë²¡í„°ë¥¼ ì‚¬ìš©í–ˆìœ¼ë‚˜, ì—¬ê¸°ì„œëŠ” í•µì‹¬ íŠ¹ì§•ë§Œ ê°„ì†Œí™”í•˜ì—¬ êµ¬í˜„
    # ì›ì ì¢…ë¥˜, ì´ì›ƒ ìˆ˜ì†Œ ì›ì ìˆ˜, í˜•ì‹ ì „í•˜, ë°©í–¥ì¡± ì—¬ë¶€ ë“±
    feature = np.zeros(14)
    feature[list(map(lambda s: s.GetSymbol(), Chem.GetPeriodicTable().GetValenceList(-1))).index(atom.GetSymbol())] = 1
    feature[10] = atom.GetTotalNumHs()
    feature[11] = atom.GetFormalCharge()
    feature[12] = atom.GetDegree()
    feature[13] = int(atom.IsInRing())
    return torch.tensor(feature, dtype=torch.float)

def get_bond_features(bond):
    """í™”í•™ ê²°í•©(Edge)ì˜ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ì—¬ ë²¡í„°ë¡œ ë³€í™˜"""
    # ê²°í•© ì¢…ë¥˜ (ë‹¨ì¼, ì´ì¤‘, ì‚¼ì¤‘, ë°©í–¥ì¡±)
    bond_type = bond.GetBondType()
    bond_map = {
        Chem.rdchem.BondType.SINGLE: [1, 0, 0, 0],
        Chem.rdchem.BondType.DOUBLE: [0, 1, 0, 0],
        Chem.rdchem.BondType.TRIPLE: [0, 0, 1, 0],
        Chem.rdchem.BondType.AROMATIC: [0, 0, 0, 1]
    }
    return torch.tensor(bond_map.get(bond_type, [0, 0, 0, 0]), dtype=torch.float)

def smiles_to_graph(smiles: str):
    """SMILES ë¬¸ìì—´ì„ PyTorch Geometric ê·¸ë˜í”„ ë°ì´í„° ê°ì²´ë¡œ ë³€í™˜"""
    mol = Chem.MolFromSmiles(smiles)
    if mol is None:
        return None

    # Node íŠ¹ì§• (ì›ì)
    atom_features = [get_atom_features(atom) for atom in mol.GetAtoms()]
    x = torch.stack(atom_features)

    # Edge Index ë° Edge íŠ¹ì§• (ê²°í•©)
    edge_indices, edge_attrs = [], []
    for bond in mol.GetBonds():
        i, j = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()
        edge_indices.append((i, j))
        edge_indices.append((j, i)) # ë¬´ë°©í–¥ ê·¸ë˜í”„
        
        bond_feat = get_bond_features(bond)
        edge_attrs.append(bond_feat)
        edge_attrs.append(bond_feat)

    edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()
    edge_attr = torch.stack(edge_attrs)
    
    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr)

# --- 2. ë”¥ëŸ¬ë‹ ëª¨ë¸ ì •ì˜ ---
# ë…¼ë¬¸ì˜ 2.7, 2.8 ì„¹ì…˜ ë° Figure 5, 6ì— í•´ë‹¹í•˜ëŠ” ë¶€ë¶„ì…ë‹ˆë‹¤.

class SMP_Stress_Predictor(nn.Module):
    def __init__(self, node_feature_dim, edge_feature_dim, temporal_feature_dim,
                 gnn_hidden_dim, num_gnn_layers, num_heads,
                 transformer_hidden_dim, num_transformer_layers, window_size):
        super(SMP_Stress_Predictor, self).__init__()
        
        self.window_size = window_size
        self.gnn_hidden_dim = gnn_hidden_dim
        self.transformer_hidden_dim = transformer_hidden_dim
        
        # Part 1: Graph Transformer (ë¶„ì êµ¬ì¡° ì¸ì½”ë”©)
        self.gnn_layers = nn.ModuleList()
        self.gnn_layers.append(TransformerConv(node_feature_dim, gnn_hidden_dim, heads=num_heads, edge_dim=edge_feature_dim))
        for _ in range(num_gnn_layers - 1):
            self.gnn_layers.append(TransformerConv(gnn_hidden_dim * num_heads, gnn_hidden_dim, heads=num_heads, edge_dim=edge_feature_dim))

        # ë™ì  ì„ë² ë”© ìƒì„±ì„ ìœ„í•œ MLP
        self.dynamic_embedding_mlp = nn.Sequential(
            nn.Linear(gnn_hidden_dim * num_heads + (temporal_feature_dim * window_size), gnn_hidden_dim),
            nn.ReLU(),
            nn.Linear(gnn_hidden_dim, gnn_hidden_dim)
        )

        # Part 2: Time Series Transformer (ì‹œê³„ì—´ ì˜ˆì¸¡)
        # ì…ë ¥ ì°¨ì›: ì‹œê³„ì—´ íŠ¹ì§• + ê·¸ë˜í”„ ì„ë² ë”©
        encoder_input_dim = temporal_feature_dim + gnn_hidden_dim
        self.input_projection = nn.Linear(encoder_input_dim, transformer_hidden_dim)
        
        # PyTorchì˜ ë‚´ì¥ Transformer ì‚¬ìš©
        transformer_layer = nn.TransformerEncoderLayer(d_model=transformer_hidden_dim, nhead=num_heads, dim_feedforward=transformer_hidden_dim*4, batch_first=True)
        self.time_series_transformer = nn.TransformerEncoder(transformer_layer, num_layers=num_transformer_layers)
        
        # ìµœì¢… ì¶œë ¥ ë ˆì´ì–´ (ì‘ë ¥ ê°’ 1ê°œ ì˜ˆì¸¡)
        self.output_layer = nn.Linear(transformer_hidden_dim, 1)

    def forward(self, epoxy_smiles_list, hardener_smiles_list, temporal_windows):
        # `temporal_windows` shape: (batch_size, window_size, temporal_features)
        
        batch_size = temporal_windows.shape[0]
        
        # --- Graph Embedding ìƒì„± ---
        epoxy_graphs = [smiles_to_graph(s) for s in epoxy_smiles_list]
        hardener_graphs = [smiles_to_graph(s) for s in hardener_smiles_list]
        
        # ì—í­ì‹œì™€ ê²½í™”ì œ ê·¸ë˜í”„ë¥¼ í•˜ë‚˜ì˜ ë°°ì¹˜ë¡œ ê²°í•© (Disconnected Graph)
        combined_graphs = [epoxy_graphs[i] for i in range(batch_size)] + [hardener_graphs[i] for i in range(batch_size)]
        graph_batch = Batch.from_data_list(combined_graphs)
        
        x, edge_index, edge_attr, batch_map = graph_batch.x, graph_batch.edge_index, graph_batch.edge_attr, graph_batch.batch
        
        # Graph Transformer ì—°ì‚°
        for layer in self.gnn_layers:
            x = F.relu(layer(x, edge_index, edge_attr))
            
        # Global Poolingìœ¼ë¡œ ê° ë¶„ì ê·¸ë˜í”„ì˜ ëŒ€í‘œ ë²¡í„° ì¶”ì¶œ
        pooled_x = global_mean_pool(x, batch_map) # Shape: (2 * batch_size, gnn_hidden_dim * num_heads)
        
        # ì—í­ì‹œì™€ ê²½í™”ì œ ì„ë² ë”©ì„ ê²°í•© (ì—¬ê¸°ì„œëŠ” í‰ê·  ì‚¬ìš©)
        epoxy_embed = pooled_x[:batch_size]
        hardener_embed = pooled_x[batch_size:]
        static_graph_embedding = (epoxy_embed + hardener_embed) / 2 # Shape: (batch_size, gnn_hidden_dim * num_heads)

        # "ë™ì " ê·¸ë˜í”„ ì„ë² ë”© ìƒì„± (ë…¼ë¬¸ ì•„ì´ë””ì–´)
        # ì •ì  ê·¸ë˜í”„ ì„ë² ë”©ì— ì‹œê³„ì—´ ìœˆë„ìš° ì •ë³´ë¥¼ ê²°í•©
        flattened_temporal = temporal_windows.view(batch_size, -1)
        dynamic_input = torch.cat([static_graph_embedding, flattened_temporal], dim=1)
        dynamic_graph_embedding = self.dynamic_embedding_mlp(dynamic_input) # Shape: (batch_size, gnn_hidden_dim)

        # --- Time Series ì˜ˆì¸¡ ---
        # Time Series Transformerì˜ ì…ë ¥ ì¤€ë¹„
        # (batch, window_size, features) í˜•íƒœë¡œ ë§Œë“¤ê¸° ìœ„í•´ ê·¸ë˜í”„ ì„ë² ë”©ì„ ë³µì œ
        expanded_graph_embedding = dynamic_graph_embedding.unsqueeze(1).repeat(1, self.window_size, 1)
        
        # ì‹œê³„ì—´ ë°ì´í„°ì™€ ê·¸ë˜í”„ ì„ë² ë”© ê²°í•©
        transformer_input = torch.cat([temporal_windows, expanded_graph_embedding], dim=2)
        
        # Transformer ì—°ì‚°
        projected_input = self.input_projection(transformer_input)
        transformer_output = self.time_series_transformer(projected_input) # Shape: (batch_size, window_size, transformer_hidden_dim)

        # ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í…ì˜ ì¶œë ¥ë§Œ ì‚¬ìš©í•˜ì—¬ ìµœì¢… ì˜ˆì¸¡
        final_step_output = transformer_output[:, -1, :]
        prediction = self.output_layer(final_step_output)
        
        return prediction.squeeze(-1)

# --- 3. í•™ìŠµ ê³¼ì • ì‹¤í–‰ ---
if __name__ == '__main__':
    # --- í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • ---
    WINDOW_SIZE = 4
    NODE_FEAT_DIM = 14 # get_atom_features ì—ì„œ ì •ì˜
    EDGE_FEAT_DIM = 4 # get_bond_features ì—ì„œ ì •ì˜
    TEMPORAL_FEAT_DIM = 3 # Time, Temperature, Length
    
    # ëª¨ë¸ íŒŒë¼ë¯¸í„°
    GNN_HIDDEN = 32
    GNN_LAYERS = 2
    TRANSFORMER_HIDDEN = 64
    TRANSFORMER_LAYERS = 2
    NUM_HEADS = 4
    
    # í•™ìŠµ íŒŒë¼ë¯¸í„°
    EPOCHS = 50
    BATCH_SIZE = 8
    LEARNING_RATE = 0.001
    
    # --- ê°€ìƒ ë°ì´í„° ìƒì„± ---
    # ì‹¤ì œë¡œëŠ” íŒŒì¼ì—ì„œ ë¡œë“œí•´ì•¼ í•©ë‹ˆë‹¤.
    print("ğŸ§ª ê°€ìƒ ë°ì´í„° ìƒì„± ì¤‘...")
    
    # ì˜ˆì œ SMILES
    epoxy_smiles = "CC(C)(C1=CC=C(C=C1)O[C@H]2COC2)C3=CC=C(C=C3)O[C@H]4COC4" # DGEBA
    hardener_smiles = "CC1(C(CC(C1)(C)CN)N)C" # IPD
    
    # 2ê°œì˜ ë‹¤ë¥¸ ì¡°í•©ì„ ì‹œë®¬ë ˆì´ì…˜
    smiles_pairs = [
        (epoxy_smiles, hardener_smiles),
        ("C1=CC=C2C(=C1)C(=O)OC2(C3=CC=C(C=C3)O[C@H]4COC4)C5=CC=C(C=C5)O[C@H]6COC6", hardener_smiles) # ë‹¤ë¥¸ ì—í­ì‹œ
    ]
    
    # ì‹œê³„ì—´ ë°ì´í„° ìƒì„± (1000 íƒ€ì„ìŠ¤í…)
    num_samples = len(smiles_pairs)
    total_timesteps = 1000
    
    all_temporal_data = torch.randn(num_samples, total_timesteps, TEMPORAL_FEAT_DIM)
    all_stress_data = torch.randn(num_samples, total_timesteps) # ì •ë‹µ ë°ì´í„°
    
    # ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°ì´í„°ì…‹ ìƒì„±
    inputs, labels, smiles_indices = [], [], []
    for i in range(num_samples):
        for t in range(total_timesteps - WINDOW_SIZE):
            inputs.append(all_temporal_data[i, t:t+WINDOW_SIZE, :])
            labels.append(all_stress_data[i, t+WINDOW_SIZE-1])
            smiles_indices.append(i)
    
    dataset_size = len(inputs)
    print(f"âœ… ì´ {dataset_size}ê°œì˜ ìœˆë„ìš° ìƒ˜í”Œ ìƒì„± ì™„ë£Œ.")
    
    # --- ëª¨ë¸ ë° í•™ìŠµ ì„¤ì • ---
    model = SMP_Stress_Predictor(
        node_feature_dim=NODE_FEAT_DIM, edge_feature_dim=EDGE_FEAT_DIM, 
        temporal_feature_dim=TEMPORAL_FEAT_DIM,
        gnn_hidden_dim=GNN_HIDDEN, num_gnn_layers=GNN_LAYERS, num_heads=NUM_HEADS,
        transformer_hidden_dim=TRANSFORMER_HIDDEN, num_transformer_layers=TRANSFORMER_LAYERS,
        window_size=WINDOW_SIZE
    )
    
    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)
    criterion = nn.MSELoss()
    
    # --- í•™ìŠµ ë£¨í”„ ---
    print("\nğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
    for epoch in range(EPOCHS):
        epoch_loss = 0
        
        # ë°ì´í„° ìˆœì„œë¥¼ ì„ì–´ì¤Œ
        shuffled_indices = torch.randperm(dataset_size)
        
        for i in range(0, dataset_size, BATCH_SIZE):
            batch_indices = shuffled_indices[i:i+BATCH_SIZE]
            
            # ë°°ì¹˜ ë°ì´í„° ì¤€ë¹„
            batch_temporal = torch.stack([inputs[j] for j in batch_indices])
            batch_labels = torch.stack([labels[j] for j in batch_indices])
            
            batch_smiles_indices = [smiles_indices[j] for j in batch_indices]
            batch_epoxy_smiles = [smiles_pairs[k][0] for k in batch_smiles_indices]
            batch_hardener_smiles = [smiles_pairs[k][1] for k in batch_smiles_indices]

            # ì˜ˆì¸¡ ë° ì†ì‹¤ ê³„ì‚°
            optimizer.zero_grad()
            predictions = model(batch_epoxy_smiles, batch_hardener_smiles, batch_temporal)
            loss = criterion(predictions, batch_labels)
            
            # ì—­ì „íŒŒ ë° ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
            loss.backward()
            optimizer.step()
            
            epoch_loss += loss.item()
            
        avg_loss = epoch_loss / (dataset_size / BATCH_SIZE)
        if (epoch + 1) % 10 == 0:
            print(f"Epoch [{epoch+1}/{EPOCHS}], Average Loss: {avg_loss:.4f}")
            
    print("ğŸ‰ í•™ìŠµ ì™„ë£Œ!")
    
    # --- í…ŒìŠ¤íŠ¸ (í•™ìŠµëœ ëª¨ë¸ë¡œ ìƒ˜í”Œ í•˜ë‚˜ ì˜ˆì¸¡) ---
    print("\nğŸ”¬ í•™ìŠµëœ ëª¨ë¸ë¡œ ìƒ˜í”Œ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸...")
    model.eval()
    with torch.no_grad():
        sample_temporal = inputs[0].unsqueeze(0) # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
        sample_epoxy = smiles_pairs[smiles_indices[0]][0]
        sample_hardener = smiles_pairs[smiles_indices[0]][1]
        
        prediction = model([sample_epoxy], [sample_hardener], sample_temporal)
        
        print(f"  - ìƒ˜í”Œ Epoxy SMILES: {sample_epoxy[:30]}...")
        print(f"  - ìƒ˜í”Œ Hardener SMILES: {sample_hardener[:30]}...")
        print(f"  - ì…ë ¥ ì‹œê³„ì—´ ë°ì´í„° (ë§ˆì§€ë§‰ ìŠ¤í…): {sample_temporal[0, -1, :].numpy()}")
        print("-" * 20)
        print(f"  ğŸ¯ ì‹¤ì œ ì‘ë ¥ ê°’: {labels[0].item():.4f}")
        print(f"  ğŸ¤– ì˜ˆì¸¡ëœ ì‘ë ¥ ê°’: {prediction.item():.4f}")